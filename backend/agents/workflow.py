"""Agentic workflow using LangGraph."""
import logging
from typing import Literal, TypedDict, Annotated
from langgraph.graph import StateGraph
from langgraph.graph.message import add_messages
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from config import settings

logger = logging.getLogger(__name__)

# Initialize LLM based on configuration
def get_llm():
    """Get LLM instance based on settings."""
    if settings.use_ollama:
        from langchain_community.llms import Ollama
        return Ollama(
            base_url=settings.ollama_base_url,
            model=settings.ollama_model,
            temperature=0.7,
        )
    else:
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(
            model=settings.openai_model, # Use settings.openai_model as in original
            temperature=0.7,
            api_key=settings.openai_api_key
        )

llm = get_llm()


class AgentState(TypedDict):
    """State for the agentic workflow."""
    messages: Annotated[list, add_messages]
    user_id: int
    intent: str | None
    context: dict


# Import agent nodes
from .task_agent import task_agent_node
from .calendar_agent import calendar_agent_node
from .reminder_agent import reminder_agent_node
from .image_agent import image_agent_node
from .document_agent import document_agent_node
from .rag_agent import rag_agent_node

# Router Node (Sync wrapper logic)
async def router_node(state: AgentState) -> AgentState:
    """
    Analyze state and Determine intent.
    This node calls the LLM to classify the intent and updates the state.
    """
    messages = state["messages"]
    last_message = messages[-1] if messages else None
    
    if not last_message or not hasattr(last_message, 'content'):
        return {**state, "intent": "general"}
        
    # Check for file upload in context
    context = state.get("context", {})
    if context.get("file_path"):
        logger.info(f"File upload detected: {context.get('file_name')}")
        return {**state, "intent": "document"}
    
    user_message = last_message.content
    
    # System prompt for intent classification
    system_prompt = """–¢—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–º–µ—Ä–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –û–ø—Ä–µ–¥–µ–ª–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–¥–Ω–æ —Å–ª–æ–≤–æ:

- task: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç —Å–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É, –æ–±–Ω–æ–≤–∏—Ç—å –∑–∞–¥–∞—á—É, –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á, —É–ø—Ä–∞–≤–ª—è—Ç—å —Å–ø–∏—Å–∫–∞–º–∏ –¥–µ–ª
- calendar: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç —Å–æ–∑–¥–∞—Ç—å —Å–æ–±—ã—Ç–∏–µ –≤ –∫–∞–ª–µ–Ω–¥–∞—Ä–µ, –≤—Å—Ç—Ä–µ—á—É, –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ-—Ç–æ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É/–≤—Ä–µ–º—è
- reminder: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –Ω–∞–ø–æ–º–Ω–∏—Ç—å –æ —á—ë–º-—Ç–æ, —Å–æ–∑–¥–∞—Ç—å –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ
- image: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –Ω–∞—Ä–∏—Å–æ–≤–∞—Ç—å, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å, —Å–æ–∑–¥–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ/–∫–∞—Ä—Ç–∏–Ω–∫—É
- document: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ –ø—Ä–æ—Å–∏—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª
- knowledge: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å, —Ç—Ä–µ–±—É—é—â–∏–π –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
- general: –æ–±—ã—á–Ω–∞—è –±–µ—Å–µ–¥–∞, –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ, –∏–ª–∏ –Ω–µ—è—Å–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ

–ü—Ä–∏–º–µ—Ä—ã:
"–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É –∫—É–ø–∏—Ç—å –º–æ–ª–æ–∫–æ" -> task
"–î–æ–±–∞–≤—å –≤—Å—Ç—Ä–µ—á—É –∑–∞–≤—Ç—Ä–∞ –≤ 15:00" -> calendar
"–ù–∞–ø–æ–º–Ω–∏ –º–Ω–µ —á–µ—Ä–µ–∑ —á–∞—Å" -> reminder
"–ù–∞—Ä–∏—Å—É–π –∫–æ—Ç–∞" -> image
"–ß—Ç–æ —Ç—ã –∑–Ω–∞–µ—à—å –æ Python?" -> knowledge
"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?" -> general

–°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: """
    
    try:
        response = await llm.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ])
        
        intent = response.content.strip().lower()
        logger.info(f"Detected intent: {intent} for message: {user_message[:50]}...")
        
        return {**state, "intent": intent}
        
    except Exception as e:
        logger.error(f"Error in intent classification: {e}", exc_info=True)
        return {**state, "intent": "general"}


async def general_response_node(state: AgentState) -> AgentState:
    """Handle general conversation."""
    messages = state["messages"]
    context = state.get("context", {})
    user_system_prompt = context.get("system_prompt", "")
    
    base_system_prompt = """–¢—ã AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç Jarvis (–î–∂–∞—Ä–≤–∏—Å). 
–¢–≤–æ—è —Ü–µ–ª—å - –±—ã—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–º –ø–æ–º–æ—â–Ω–∏–∫–æ–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–¢–í–û–ò –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
1. üìù **–ó–∞–¥–∞—á–∏ –∏ —Å–ø–∏—Å–∫–∏**: –°–æ–∑–¥–∞—é, –æ—Ç—Å–ª–µ–∂–∏–≤–∞—é –∏ —É–ø—Ä–∞–≤–ª—è—é –∑–∞–¥–∞—á–∞–º–∏ (Todo).
   - –ü—Ä–∏–º–µ—Ä: "–°–æ–∑–¥–∞–π –∑–∞–¥–∞—á—É –∫—É–ø–∏—Ç—å —Ö–ª–µ–±", "–î–æ–±–∞–≤—å –≤ —Å–ø–∏—Å–æ–∫ –ø–æ–∫—É–ø–æ–∫ –º–æ–ª–æ–∫–æ".
   
2. üìÖ **–ö–∞–ª–µ–Ω–¥–∞—Ä—å –∏ –≤—Å—Ç—Ä–µ—á–∏**: –ü–ª–∞–Ω–∏—Ä—É—é —Å–æ–±—ã—Ç–∏—è –∏ –≤—Å—Ç—Ä–µ—á–∏.
   - –ü—Ä–∏–º–µ—Ä: "–ó–∞–ø–ª–∞–Ω–∏—Ä—É–π –≤—Å—Ç—Ä–µ—á—É –∑–∞–≤—Ç—Ä–∞ –≤ 14:00 —Å –∫–æ–º–∞–Ω–¥–æ–π".

3. ‚è∞ **–ù–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è**: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è.
   - –ü—Ä–∏–º–µ—Ä: "–ù–∞–ø–æ–º–Ω–∏ —á–µ—Ä–µ–∑ 15 –º–∏–Ω—É—Ç –≤—ã–∫–ª—é—á–∏—Ç—å –¥—É—Ö–æ–≤–∫—É".

4. üìö **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (RAG)**: –£–º–µ—é —á–∏—Ç–∞—Ç—å –∏ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã.
   - –ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å –º–Ω–µ PDF, DOCX –∏–ª–∏ TXT —Ñ–∞–π–ª.
   - –ü–æ—Ç–æ–º —Å–ø—Ä–æ—Å–∏: "–ß—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω–æ –≤ –¥–æ–≥–æ–≤–æ—Ä–µ?", "–ù–∞–π–¥–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–µ–∫—Ç–µ".

5. üñºÔ∏è **–†–∞–±–æ—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏**: (–í —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ) –£–º–µ—é –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ –æ–ø–∏—Å—ã–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏.

–°–¢–†–£–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏, –µ—Å–ª–∏ —Å–ø—Ä–æ—Å—è—Ç):
–Ø –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ:
- **Backend**: Python, LangGraph (–∞–≥–µ–Ω—Ç—ã), FastAPI.
- **–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö**: PostgreSQL (—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö), Qdrant (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º).
- **Frontend**: Next.js (–≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è).
- **Integrations**: Telegram Bot, OpenAI GPT-4, Local LLM (Ollama).

–û—Ç–≤–µ—á–∞–π –≤–µ–∂–ª–∏–≤–æ, –∫–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É. –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏.
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç "—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å?", –∏—Å–ø–æ–ª—å–∑—É–π —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≤—ã—à–µ."""

    # Append user's custom prompt if available
    full_system_prompt = base_system_prompt
    if user_system_prompt:
        full_system_prompt += f"\n\n–í–ê–ñ–ù–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò –û–¢ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:\n{user_system_prompt}"
    
    try:
        response = await llm.ainvoke([
            SystemMessage(content=full_system_prompt),
            *messages
        ])
        
        return {
            **state,
            "messages": [response]
        }
    except Exception as e:
        logger.error(f"Error in general response: {e}", exc_info=True)
        return {
            **state,
            "messages": [AIMessage(content="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑.")]
        }


# Conditional edge function (must be sync)
def route_to_agent(state: AgentState) -> str:
    """Route to appropriate agent based on state intent."""
    intent = state.get("intent", "general")
    
    intent_mapping = {
        "task": "task_agent",
        "calendar": "calendar_agent",
        "reminder": "reminder_agent",
        "image": "image_agent",
        "document": "document_agent",
        "knowledge": "rag_agent",
        "general": "general_response",
    }
    
    return intent_mapping.get(intent, "general_response")


# Build the workflow graph
def build_workflow() -> StateGraph:
    """Build and compile the LangGraph workflow."""
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("router", router_node)
    workflow.add_node("general_response", general_response_node)
    
    # Add agent nodes
    workflow.add_node("task_agent", task_agent_node)
    workflow.add_node("calendar_agent", calendar_agent_node)
    workflow.add_node("reminder_agent", reminder_agent_node)
    workflow.add_node("image_agent", image_agent_node)
    workflow.add_node("document_agent", document_agent_node)
    workflow.add_node("rag_agent", rag_agent_node)
    
    # Set entry point
    workflow.set_entry_point("router")
    
    # Conditional routing from router
    workflow.add_conditional_edges(
        "router",
        route_to_agent,
        {
            "task_agent": "task_agent",
            "calendar_agent": "calendar_agent",
            "reminder_agent": "reminder_agent",
            "image_agent": "image_agent",
            "document_agent": "document_agent",
            "rag_agent": "rag_agent",
            "general_response": "general_response",
        }
    )
    
    # Set finish points for all agents
    workflow.set_finish_point("task_agent")
    workflow.set_finish_point("calendar_agent")
    workflow.set_finish_point("reminder_agent")
    workflow.set_finish_point("image_agent")
    workflow.set_finish_point("document_agent")
    workflow.set_finish_point("rag_agent")
    workflow.set_finish_point("general_response")
    
    # Compile the workflow
    return workflow.compile()

# Global workflow instance
agent_workflow = build_workflow()


async def process_message(user_id: int, message: str, context: dict = None) -> str:
    """
    Process a user message through the agentic workflow.
    
    Args:
        user_id: Telegram user ID
        message: User's message
        context: Optional context dictionary
        
    Returns:
        AI assistant's response
    """
    initial_state = {
        "messages": [HumanMessage(content=message)],
        "user_id": user_id,
        "intent": None,
        "context": context or {},
    }
    
    try:
        result = await agent_workflow.ainvoke(initial_state)
        
        # Extract the last AI message
        messages = result.get("messages", [])
        if messages:
            last_message = messages[-1]
            if hasattr(last_message, 'content'):
                return last_message.content
        
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–∞—à –∑–∞–ø—Ä–æ—Å."
        
    except Exception as e:
        logger.error(f"Error processing message: {e}", exc_info=True)
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
